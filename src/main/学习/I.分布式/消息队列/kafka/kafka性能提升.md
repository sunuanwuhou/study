# Table of Contents

* [增加分片与消费者数量](#增加分片与消费者数量)
* [保证分片内写入顺序](#保证分片内写入顺序)
* [单消费者速度提升](#单消费者速度提升)


先简单说下遇到的业务场景：

> 一个互动类的论坛的帖子评论处理场景，要求每个帖子的评论请求操作都必须要严格遵循一定的顺序（比如可能会有评论删除、引用评论、回复评论等操作，所以请求顺序必须要严格按照顺序处理），**帖子评论的操作请求发送到kafka里面，然后评论服务消费kafka处理各个请求**，这个评论消费者服务消费太慢，需要提升下并发效率。





# 增加分片与消费者数量

首先是**常规调整**：**根据kafka自身的机制，将topic进行分片调整，拆分为N个分片，然后增设消费者组，在消费者组内部署与分片数相等的消费者服务节点**，这样每个消费者可以处理一个分片，这样整个评论的消费性能就会提升N倍。

> 注意是：消费者分区数=生产者分区数
>
> 注册的时候，会有默认的分配策略
>
> - 消费者数量太少，会导致一个消费者需要消费多个分片的数据，造成某一个消费者消费压力提升；
> - 消费者数量太多，会导致有的消费者并不会消费任何数据，浪费部署资源。



# 保证分片内写入顺序

生产者写入消息到kafka的topic时，kafka将依据不同的策略将数据分配到不同的分区中：

> 1. 轮询分区策略
> 2. 随机分区策略
> 3. 按key分区分配策略
> 4. 自定义分区策略

这里采用自定义分区策略，因为每个评论操作请求中都携带有一个原始帖子ID字段，所以分发策略也很简单，直接`帖子ID % 分片数`将消息进行分发，这样同一个帖子ID的评论操作就都可以到同一个分片中，这样顺序的问题就解决了。



> kakfa是分区有序，如果想做到全局有序，
>
> 我能想到有2种解决方案。
>
> + 只用一个分区
> + 数据做标识，消费者持久到本地，再去根据标记组合来达到目的。
> + 如果只有一个消费者，可以用队列的方式，同第二种方式，不过消息不能持久化。



# 单消费者速度提升

在前面方案的基础上，主要是对消费者端的实现逻辑进行了调整：

- 在消费者内部，区分`Consumer Thread`和`Work Thread`，`Consumer Thread`负责从kafka拉取消息，而`Work Thread`负责真正的消费逻辑处理。
- 单机内存中维护若干个队列，每个队列对应一个Work Thread，负责消费该队列中的数据；
- `Consumer Thread`基于`亲缘性分发策略`对消息进行二次分发，保证相同帖子ID的请求分发到相同的内部队列中。


我们目前，是持久化到本地，在去消费数据，消费数据的时候，可以指定每台服务消费的策略。
